{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.data_anonymizer import PresidioReversibleAnonymizer\n",
    "from presidio_analyzer import Pattern, PatternRecognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class PIIAgent:\n",
    "    \"\"\"\n",
    "    A class to mask Personally Identifiable Information (PII) in an email thread.\n",
    "\n",
    "    This class encapsulates the functionality to anonymize and deanonymize Personally Identifiable Information (PII)\n",
    "    using custom patterns for various entities such as credit card numbers, account numbers, CIF numbers,\n",
    "    UAE phone numbers, and Emirates IDs. It uses the inbuilt entities for Email Addresses, Locations, Person Names and Date Times.\n",
    "\n",
    "    A new anonymizer instance is initialized every time the class is instantiated.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize the anonymizer\n",
    "        self.anonymizer = PresidioReversibleAnonymizer(\n",
    "            analyzed_fields=[\n",
    "                \"IBAN_CODE\",\n",
    "                \"EMAIL_ADDRESS\",\n",
    "                \"PERSON\",\n",
    "                \"LOCATION\",\n",
    "                \"DATE_TIME\",\n",
    "            ],\n",
    "            add_default_faker_operators=False,\n",
    "        )\n",
    "        # Add recognizers\n",
    "        self._add_recognizers()\n",
    "\n",
    "    def _add_recognizers(self):\n",
    "        # Add custom recognizers to the anonymizer\n",
    "\n",
    "        ## Credit Card Number\n",
    "        credit_card_number_pattern = Pattern(\n",
    "            name=\"credit_card_number_pattern\",\n",
    "            regex=r\"\\b\\d{16}\\b\",\n",
    "            score=1,\n",
    "        )\n",
    "        credit_card_number_recognizer = PatternRecognizer(\n",
    "            supported_entity=\"CREDIT_CARD_NUMBER\", patterns=[credit_card_number_pattern]\n",
    "        )\n",
    "        self.anonymizer.add_recognizer(credit_card_number_recognizer)\n",
    "\n",
    "        ## Account Number\n",
    "        account_number_pattern = Pattern(\n",
    "            name=\"account_number_pattern\",\n",
    "            regex=r\"\\b120\\d{8}\\b\",\n",
    "            score=1,\n",
    "        )\n",
    "        account_number_recognizer = PatternRecognizer(\n",
    "            supported_entity=\"ACCOUNT_NUMBER\", patterns=[account_number_pattern]\n",
    "        )\n",
    "        self.anonymizer.add_recognizer(account_number_recognizer)\n",
    "\n",
    "        ## CIF Number\n",
    "        cif_number_pattern = Pattern(\n",
    "            name=\"cif_number_pattern\",\n",
    "            regex=r\"\\b\\d{6}\\b\",\n",
    "            score=1,\n",
    "        )\n",
    "        cif_number_recognizer = PatternRecognizer(\n",
    "            supported_entity=\"CIF_NUMBER\", patterns=[cif_number_pattern]\n",
    "        )\n",
    "        self.anonymizer.add_recognizer(cif_number_recognizer)\n",
    "\n",
    "        ## UAE Phone Number\n",
    "        phone_number_pattern = Pattern(\n",
    "            name=\"phone_number_pattern\",\n",
    "            regex=r\"(?:\\+971|00971|971)[\\s\\-]?5[\\s\\-]?\\d{1}[\\s\\-]?\\d{3}[\\s\\-]?\\d{4}\",\n",
    "            score=1,\n",
    "        )\n",
    "        phone_number_recognizer = PatternRecognizer(\n",
    "            supported_entity=\"UAE_PHONE_NUMBER\", patterns=[phone_number_pattern]\n",
    "        )\n",
    "        self.anonymizer.add_recognizer(phone_number_recognizer)\n",
    "\n",
    "        ## Emirates ID\n",
    "        emirates_id_pattern = Pattern(\n",
    "            name=\"emirates_id_pattern\",\n",
    "            regex=r\"784-?\\d{4}-?\\d{7}-?\\d\",\n",
    "            score=1,\n",
    "        )\n",
    "        emirates_id_recognizer = PatternRecognizer(\n",
    "            supported_entity=\"EMIRATES_ID\", patterns=[emirates_id_pattern]\n",
    "        )\n",
    "        self.anonymizer.add_recognizer(emirates_id_recognizer)\n",
    "\n",
    "    def mask(self, text):\n",
    "        # Anonymize the given text\n",
    "        return self.anonymizer.anonymize(text)\n",
    "\n",
    "    def unmask(self, anonymized_text):\n",
    "        # Deanonymize the given text\n",
    "        return self.anonymizer.deanonymize(anonymized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\miniconda3\\envs\\idk\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    Context: \n",
      "   From: <PERSON> <<EMAIL_ADDRESS><mailto:<EMAIL_ADDRESS>>>\n",
      "Date: <DATE_TIME>, <DATE_TIME_2> at <DATE_TIME_3>\n",
      "To: \"<PERSON_2>.<PERSON_3>\" <<PERSON_2>.<PERSON_3>@nbf.ae<mailto:<PERSON_2>.<PERSON_3>@nbf.ae>>\n",
      "Cc: <PERSON_4> <<EMAIL_ADDRESS_3><mailto:<EMAIL_ADDRESS_3>>>, <PERSON_5>>>\n",
      "Subject: Re: Secure Email Message\n",
      "\n",
      "\n",
      "\n",
      "Dear <PERSON_2>\n",
      "\n",
      "\n",
      "\n",
      "Attached Invoice for which payment US$ 262,845 has be remitted .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Best Regards\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "    Customer Support Ticket #12345\n",
      "    Name: <PERSON>Email: <EMAIL_ADDRESS>\n",
      "    Phone: <UAE_PHONE_NUMBER>\n",
      "    Emirates ID: <EMIRATES_ID>\n",
      "    Account: 1201234567\n",
      "    \n",
      "    Customer reported issues with recent transactions on their credit card <CREDIT_CARD_NUMBER>.\n",
      "    Follow-up required by <DATE_TIME>.\n",
      "    \n",
      "\n",
      "\n",
      "    Customer Support Ticket #12345\n",
      "    Name: <PERSON>Email: <EMAIL_ADDRESS>\n",
      "    Phone: <UAE_PHONE_NUMBER>\n",
      "    Emirates ID: <EMIRATES_ID>\n",
      "    Account: 1201234567\n",
      "    \n",
      "    Customer reported issues with recent transactions on their credit card <CREDIT_CARD_NUMBER>.\n",
      "    Follow-up required by <DATE_TIME>.\n",
      "    \n",
      "\n",
      "\n",
      "    Customer Support Ticket #12345\n",
      "    Name: <PERSON>Email: <EMAIL_ADDRESS>\n",
      "    Phone: <UAE_PHONE_NUMBER>\n",
      "    Emirates ID: <EMIRATES_ID>\n",
      "    Account: 1201234567\n",
      "    \n",
      "    Customer reported issues with recent transactions on their credit card <CREDIT_CARD_NUMBER>.\n",
      "    Follow-up required by <DATE_TIME>.\n",
      "    \n",
      "    \n",
      "    Question: What is payment amount for the attached invoice?\n",
      "    \n",
      "    Please provide a professional and concise response based on the given context.\n",
      "    Focus only on the relevant information and maintain a helpful tone.\n",
      "    \n",
      "    Answer:\n",
      "    \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\miniconda3\\envs\\idk\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:394: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Query: What is payment amount for the attached invoice?\n",
      "Response: The payment amount for the attached invoice is US$ 262,845.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class RAGWithPIIApplication:\n",
    "    \"\"\"\n",
    "    A RAG application that handles PII data safely by anonymizing content before storage\n",
    "    and deanonymizing it during retrieval, using Google's Gemini API for generation.\n",
    "    \"\"\"\n",
    "    \n",
    "<<<<<<< Tabnine <<<<<<<\n",
    "    def __init__(#-\n",
    "        self, #-\n",
    "        persist_directory: str = \"./chroma_db\",#-\n",
    "        model_name: str = \"gemini-pro\",#-\n",
    "        temperature: float = 0.1,#-\n",
    "        top_p: float = 0.8,#-\n",
    "        top_k: int = 40,#-\n",
    "        max_output_tokens: int = 2048#-\n",
    "    ):#-\n",
    "    class RAGWithPIIApplication:#+\n",
    "        \"\"\"\n",
    "        Initialize the RAG application with PII handling capabilities.#-\n",
    "#-\n",
    "        Args:#-\n",
    "            persist_directory (str): Directory to persist the vector store#-\n",
    "            model_name (str): Gemini model name to use#-\n",
    "            temperature (float): Sampling temperature for generation#-\n",
    "            top_p (float): Nucleus sampling parameter#-\n",
    "            top_k (int): Top-k sampling parameter#-\n",
    "            max_output_tokens (int): Maximum number of tokens in the generated response#-\n",
    "        A RAG application that handles PII data safely by anonymizing content before storage#+\n",
    "        and deanonymizing it during retrieval, using Google's Gemini API for generation.#+\n",
    "        \"\"\"\n",
    "\n",
    "        self.pii_agent = PIIAgent()#-\n",
    "        self.embeddings = HuggingFaceEmbeddings(#-\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"#-\n",
    "        )#-\n",
    "        def __init__(#+\n",
    "            self, #+\n",
    "            persist_directory: str = \"./chroma_db\",#+\n",
    "            model_name: str = \"gemini-pro\",#+\n",
    "            temperature: float = 0.1,#+\n",
    "            top_p: float = 0.8,#+\n",
    "            top_k: int = 40,#+\n",
    "            max_output_tokens: int = 2048#+\n",
    "        ):#+\n",
    "            \"\"\"#+\n",
    "            Initialize the RAG application with PII handling capabilities.#+\n",
    "\n",
    "            Args:#+\n",
    "                persist_directory (str): Directory to persist the vector store#+\n",
    "                model_name (str): Gemini model name to use#+\n",
    "                temperature (float): Sampling temperature for generation#+\n",
    "                top_p (float): Nucleus sampling parameter#+\n",
    "                top_k (int): Top-k sampling parameter#+\n",
    "                max_output_tokens (int): Maximum number of tokens in the generated response#+\n",
    "            \"\"\"#+\n",
    "\n",
    "        self.llm = ChatGoogleGenerativeAI(#-\n",
    "            model=model_name,#-\n",
    "            temperature=temperature,#-\n",
    "            top_p=top_p,#-\n",
    "            top_k=top_k,#-\n",
    "            max_output_tokens=max_output_tokens,#-\n",
    "            convert_system_message_to_human=True,api_key=\"\"#-\n",
    "        )#-\n",
    "            self.pii_agent = PIIAgent()#+\n",
    "            self.embeddings = HuggingFaceEmbeddings(#+\n",
    "                model_name=\"sentence-transformers/all-MiniLM-L6-v2\"#+\n",
    "            )#+\n",
    "\n",
    "\n",
    "        self.persist_directory = persist_directory#-\n",
    "        self.vector_store = Chroma(#-\n",
    "            persist_directory=persist_directory,#-\n",
    "            embedding_function=self.embeddings#-\n",
    "        )#-\n",
    "            self.llm = ChatGoogleGenerativeAI(#+\n",
    "                model=model_name,#+\n",
    "                temperature=temperature,#+\n",
    "                top_p=top_p,#+\n",
    "                top_k=top_k,#+\n",
    "                max_output_tokens=max_output_tokens,#+\n",
    "                convert_system_message_to_human=True,api_key=\"AIzaSyBcS2IyK2LdE5EeVDXLBUNCztj09G25Be0\"#+\n",
    "            )#+\n",
    "\n",
    "\n",
    "        self.text_mappings: Dict[str, Tuple[str, str]] = {}#-\n",
    "            self.persist_directory = persist_directory#+\n",
    "            self.vector_store = Chroma(#+\n",
    "                persist_directory=persist_directory,#+\n",
    "                embedding_function=self.embeddings#+\n",
    "            )#+\n",
    "\n",
    "\n",
    "        self.qa_prompt = PromptTemplate(#-\n",
    "            input_variables=[\"context\", \"question\"],#-\n",
    "            template=\"\"\"#-\n",
    "            Context: {context}#-\n",
    "            self.text_mappings: Dict[str, Tuple[str, str]] = {}#+\n",
    "\n",
    "            Question: {question}#-\n",
    "\n",
    "            Please provide a clear and concise answer based on the context above. If the information isn't available in the context, please say so.#-\n",
    "            self.qa_prompt = PromptTemplate(#+\n",
    "                input_variables=[\"context\", \"question\"],#+\n",
    "                template=\"\"\"#+\n",
    "                Context: {context}#+\n",
    "\n",
    "            Answer:\"\"\"#-\n",
    "        )#-\n",
    "                Question: {question}#+\n",
    "#+\n",
    "                Please provide a clear and concise answer based on the context above. If the information isn't available in the context, please say so.#+\n",
    "#+\n",
    "                Answer:\"\"\"#+\n",
    "            )#+\n",
    ">>>>>>> Tabnine >>>>>>># {\"conversationId\":\"03485467-301c-40eb-ba8c-f77572ce9510\",\"source\":\"instruct\"}\n",
    "        \n",
    "    def process_documents(self, documents_dir: str, batch_size: int = 100) -> None:\n",
    "        \"\"\"Process documents from a directory, anonymize PII, and store in vector database.\"\"\"\n",
    "        loader = DirectoryLoader(\n",
    "            documents_dir,\n",
    "            glob=\"**/*.txt\",\n",
    "            loader_cls=TextLoader\n",
    "        )\n",
    "        documents = loader.load()\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        splits = text_splitter.split_documents(documents)\n",
    "        \n",
    "        for i in range(0, len(splits), batch_size):\n",
    "            batch = splits[i:i + batch_size]\n",
    "            self._process_batch(batch)\n",
    "            \n",
    "        self.vector_store.persist()\n",
    "        \n",
    "    def _process_batch(self, documents: List[Document]) -> None:\n",
    "        \"\"\"Process a batch of documents by anonymizing PII and storing in vector store.\"\"\"\n",
    "        anonymized_docs = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            anonymized_text = self.pii_agent.mask(doc.page_content)\n",
    "            doc_id = hash(doc.page_content)\n",
    "            self.text_mappings[doc_id] = (doc.page_content, anonymized_text)\n",
    "            \n",
    "            anonymized_doc = Document(\n",
    "                page_content=anonymized_text,\n",
    "                metadata={\n",
    "                    **doc.metadata,\n",
    "                    \"doc_id\": doc_id\n",
    "                }\n",
    "            )\n",
    "            anonymized_docs.append(anonymized_doc)\n",
    "        \n",
    "        self.vector_store.add_documents(anonymized_docs)\n",
    "        \n",
    "    def query(\n",
    "        self, \n",
    "        query: str, \n",
    "        k: int = 4,\n",
    "        system_prompt: str = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Query the RAG system with automatic PII handling using Gemini.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User query\n",
    "            k (int): Number of relevant documents to retrieve\n",
    "            system_prompt (str, optional): System prompt for Gemini\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated response with deanonymized content\n",
    "        \"\"\"\n",
    "    \n",
    "        anonymized_query = self.pii_agent.mask(query)\n",
    "        \n",
    "    \n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.vector_store.as_retriever(\n",
    "                search_kwargs={\"k\": k}\n",
    "            ),\n",
    "            chain_type_kwargs={\n",
    "                \"prompt\": self.qa_prompt,\n",
    "                \"verbose\": True\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    \n",
    "        response = qa_chain.run(anonymized_query)\n",
    "        \n",
    "    \n",
    "        deanonymized_response = self.pii_agent.unmask(response)\n",
    "        \n",
    "        return deanonymized_response\n",
    "    \n",
    "    def add_single_document(self, content: str, metadata: Dict = None) -> None:\n",
    "        \"\"\"Add a single document to the RAG system.\"\"\"\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "            \n",
    "        anonymized_content = self.pii_agent.mask(content)\n",
    "        doc_id = hash(content)\n",
    "        self.text_mappings[doc_id] = (content, anonymized_content)\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content=anonymized_content,\n",
    "            metadata={\n",
    "                **metadata,\n",
    "                \"doc_id\": doc_id\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.vector_store.add_documents([doc])\n",
    "        self.vector_store.persist()\n",
    "\n",
    "    def set_custom_prompt(self, template: str, input_variables: List[str] = None) -> None:\n",
    "        \"\"\"\n",
    "        Set a custom prompt template for the QA chain.\n",
    "        \n",
    "        Args:\n",
    "            template (str): The prompt template string\n",
    "            input_variables (List[str], optional): List of input variables in the template.\n",
    "                          Defaults to [\"context\", \"question\"]\n",
    "        \"\"\"\n",
    "        if input_variables is None:\n",
    "            input_variables = [\"context\", \"question\"]\n",
    "            \n",
    "        self.qa_prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=input_variables\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_dotenv()  \n",
    "    \n",
    "    \n",
    "    rag_app = RAGWithPIIApplication()\n",
    "    \n",
    "    sample_doc = \"\"\"\n",
    "   From: Sanjay Wariyar <sanjay.wariyar@silco.ae<mailto:sanjay.wariyar@silco.ae>>\n",
    "Date: Thursday, 6 June 2024 at 12:05 PM\n",
    "To: \"Afroze.Naseem\" <Afroze.Naseem@nbf.ae<mailto:Afroze.Naseem@nbf.ae>>\n",
    "Cc: Anupam Paul <anupam.paul@silco.ae<mailto:anupam.paul@silco.ae>>, Saiu George <saju@silco.ae<mailto:saju@silco.ae>>\n",
    "Subject: Re: Secure Email Message\n",
    "\n",
    "\n",
    "\n",
    "Dear Afroze\n",
    "\n",
    "\n",
    "\n",
    "Attached Invoice for which payment US$ 262,845 has be remitted .\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Best Regards\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    rag_app.add_single_document(sample_doc)\n",
    "    \n",
    "    \n",
    "    custom_prompt = \"\"\"\n",
    "    Context: {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Please provide a professional and concise response based on the given context.\n",
    "    Focus only on the relevant information and maintain a helpful tone.\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    rag_app.set_custom_prompt(custom_prompt)\n",
    "    \n",
    "    \n",
    "    query = \"What is payment amount for the attached invoice?\"\n",
    "    response = rag_app.query(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.data_anonymizer import PresidioReversibleAnonymizer\n",
    "from presidio_analyzer import Pattern, PatternRecognizer\n",
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "class PIIAgent:\n",
    "    def __init__(self):\n",
    "        self.anonymizer = PresidioReversibleAnonymizer(\n",
    "            analyzed_fields=[\n",
    "                \"IBAN_CODE\",\n",
    "                \"EMAIL_ADDRESS\",\n",
    "                \"PERSON\",\n",
    "                \"LOCATION\",\n",
    "                \"DATE_TIME\",\n",
    "            ],\n",
    "            add_default_faker_operators=False,\n",
    "        )\n",
    "        self._add_recognizers()\n",
    "\n",
    "    def _add_recognizers(self):\n",
    "        credit_card_number_pattern = Pattern(\n",
    "            name=\"credit_card_number_pattern\",\n",
    "            regex=r\"\\b\\d{16}\\b\",\n",
    "            score=1,\n",
    "        )\n",
    "        credit_card_number_recognizer = PatternRecognizer(\n",
    "            supported_entity=\"CREDIT_CARD_NUMBER\", patterns=[credit_card_number_pattern]\n",
    "        )\n",
    "        self.anonymizer.add_recognizer(credit_card_number_recognizer)\n",
    "\n",
    "        account_number_pattern = Pattern(\n",
    "            name=\"account_number_pattern\",\n",
    "            regex=r\"\\b120\\d{8}\\b\",\n",
    "            score=1,\n",
    "        )\n",
    "        account_number_recognizer = PatternRecognizer(\n",
    "            supported_entity=\"ACCOUNT_NUMBER\", patterns=[account_number_pattern]\n",
    "        )\n",
    "        self.anonymizer.add_recognizer(account_number_recognizer)\n",
    "\n",
    "        cif_number_pattern = Pattern(\n",
    "            name=\"cif_number_pattern\",\n",
    "            regex=r\"\\b\\d{6}\\b\",\n",
    "            score=1,\n",
    "        )\n",
    "        cif_number_recognizer = PatternRecognizer(\n",
    "            supported_entity=\"CIF_NUMBER\", patterns=[cif_number_pattern]\n",
    "        )\n",
    "        self.anonymizer.add_recognizer(cif_number_recognizer)\n",
    "\n",
    "        phone_number_pattern = Pattern(\n",
    "            name=\"phone_number_pattern\",\n",
    "            regex=r\"(?:\\+971|00971|971)[\\s\\-]?5[\\s\\-]?\\d{1}[\\s\\-]?\\d{3}[\\s\\-]?\\d{4}\",\n",
    "            score=1,\n",
    "        )\n",
    "        phone_number_recognizer = PatternRecognizer(\n",
    "            supported_entity=\"UAE_PHONE_NUMBER\", patterns=[phone_number_pattern]\n",
    "        )\n",
    "        self.anonymizer.add_recognizer(phone_number_recognizer)\n",
    "\n",
    "        emirates_id_pattern = Pattern(\n",
    "            name=\"emirates_id_pattern\",\n",
    "            regex=r\"784-?\\d{4}-?\\d{7}-?\\d\",\n",
    "            score=1,\n",
    "        )\n",
    "        emirates_id_recognizer = PatternRecognizer(\n",
    "            supported_entity=\"EMIRATES_ID\", patterns=[emirates_id_pattern]\n",
    "        )\n",
    "        self.anonymizer.add_recognizer(emirates_id_recognizer)\n",
    "\n",
    "    def mask(self, text):\n",
    "        return self.anonymizer.anonymize(text)\n",
    "\n",
    "    def unmask(self, anonymized_text):\n",
    "        return self.anonymizer.deanonymize(anonymized_text)\n",
    "\n",
    "class RAGPIIChatBot:\n",
    "    def __init__(self):\n",
    "        # Initialize LLM with Google Gemini\n",
    "        self.llm = ChatGoogleGenerativeAI(model = \"gemini-pro\",api_key=\"AIzaSyBcS2IyK2LdE5EeVDXLBUNCztj09G25Be0\")\n",
    "\n",
    "        # Initialize open-source embeddings (e.g., SentenceTransformer)\n",
    "        self.embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "        # Load or create FAISS index for vector storage\n",
    "        try:\n",
    "            self.vectorstore = FAISS.load_local(\"faiss_index\", embeddings=self.embeddings)\n",
    "        except Exception:\n",
    "            documents = []  # Initialize an empty list if no documents are preloaded\n",
    "            self.vectorstore = FAISS.from_documents(documents, self.embeddings)\n",
    "            self.vectorstore.save_local(\"faiss_index\")  # Save the index for future use\n",
    "\n",
    "        # Initialize retriever\n",
    "        self.retriever = self.vectorstore.as_retriever()\n",
    "\n",
    "        # Initialize conversation chain with the LLM and retriever\n",
    "        self.qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=self.llm, retriever=self.retriever\n",
    "        )\n",
    "\n",
    "        # PII Agent for masking and unmasking PII data\n",
    "        self.pii_agent = PIIAgent()\n",
    "\n",
    "    def run(self, query, chat_history=[]):\n",
    "        masked_query = self.pii_agent.mask(query)\n",
    "        response = self.qa_chain({\"question\": masked_query, \"chat_history\": chat_history})\n",
    "        return self.pii_agent.unmask(response[\"answer\"])\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    rag_bot = RAGPIIChatBot()\n",
    "    user_query = \"What is the status of the payment for the account 1201234567?\"\n",
    "    response = rag_bot.run(user_query)\n",
    "    print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
